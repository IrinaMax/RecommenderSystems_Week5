{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a song recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load music data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#song_sf = graphlab.SFrame('song_data.gl/')\n",
    "#song_sf.save('song_data.csv', format = 'csv')\n",
    "song_df =  pandas.read_csv('song_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data\n",
    "\n",
    "Music data shows how many times a user listened to a song, as well as the details of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(song_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "song_df = song_df.head(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the most popular songs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_grouped = song_df.groupby(['song']).agg({'listen_count': 'count'}).reset_index()\n",
    "grouped_sum = song_grouped['listen_count'].sum()\n",
    "song_grouped['percentage']  = song_grouped['listen_count'].div(grouped_sum)*100\n",
    "song_grouped.sort_values(['listen_count', 'song'], ascending = [0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of unique users in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users = song_df['user_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 1. Count the number of unique songs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Fill in the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a song recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(song_df, test_size = 0.20, random_state=0)\n",
    "print(train_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple popularity-based recommender class (Can be used as a black box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Class for Popularity based Recommender System model\n",
    "class popularity_recommender_py():\n",
    "    def __init__(self):\n",
    "        self.train_data = None\n",
    "        self.user_id = None\n",
    "        self.item_id = None\n",
    "        self.popularity_recommendations = None\n",
    "        \n",
    "    #Create the popularity based recommender system model\n",
    "    def create(self, train_data, user_id, item_id):\n",
    "        self.train_data = train_data\n",
    "        self.user_id = user_id\n",
    "        self.item_id = item_id\n",
    "\n",
    "        #Get a count of user_ids for each unique song as recommendation score\n",
    "        train_data_grouped = train_data.groupby([self.item_id]).agg({self.user_id: 'count'}).reset_index()\n",
    "        train_data_grouped.rename(columns = {'user_id': 'score'},inplace=True)\n",
    "    \n",
    "        #Sort the songs based upon recommendation score\n",
    "        train_data_sort = train_data_grouped.sort_values(['score', self.item_id], ascending = [0,1])\n",
    "    \n",
    "        #Generate a recommendation rank based upon score\n",
    "        train_data_sort['Rank'] = train_data_sort['score'].rank(ascending=0, method='first')\n",
    "        \n",
    "        #Get the top 10 recommendations\n",
    "        self.popularity_recommendations = train_data_sort.head(10)\n",
    "\n",
    "    #Use the popularity based recommender system model to\n",
    "    #make recommendations\n",
    "    def recommend(self, user_id):    \n",
    "        user_recommendations = self.popularity_recommendations\n",
    "        \n",
    "        #Add user_id column for which the recommendations are being generated\n",
    "        user_recommendations['user_id'] = user_id\n",
    "    \n",
    "        #Bring user_id column to the front\n",
    "        cols = user_recommendations.columns.tolist()\n",
    "        cols = cols[-1:] + cols[:-1]\n",
    "        user_recommendations = user_recommendations[cols]\n",
    "        \n",
    "        return user_recommendations\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of popularity based recommender class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pm = popularity_recommender_py()\n",
    "pm.create(train_data, 'user_id', 'song')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the popularity model to make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_id = '04396079bfe2a35ee92522dfadf2056ef899c456'\n",
    "pm.recommend(user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 2: Use the popularity based model to make predictions for the following user id (Note the difference in recommendations from the first user id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_id = 'b060d8ee0a018bc167f2feaaf9f50d5c84ac6ae4'\n",
    "###Fill in the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a song recommender with personalization\n",
    "\n",
    "We now create an item similarity based collaborative filtering model that allows us to make personalized recommendations to each user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for an item similarity based personalized recommender system (Can be used as a black box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Class for Item similarity based Recommender System model\n",
    "class item_similarity_recommender_py():\n",
    "    def __init__(self):\n",
    "        self.train_data = None\n",
    "        self.user_id = None\n",
    "        self.item_id = None\n",
    "        self.cooccurence_matrix = None\n",
    "        self.songs_dict = None\n",
    "        self.rev_songs_dict = None\n",
    "        self.item_similarity_recommendations = None\n",
    "        \n",
    "    #Get unique items (songs) corresponding to a given user\n",
    "    def get_user_items(self, user):\n",
    "        user_data = self.train_data[self.train_data[self.user_id] == user]\n",
    "        user_items = list(user_data[self.item_id].unique())\n",
    "        \n",
    "        return user_items\n",
    "        \n",
    "    #Get unique users for a given item (song)\n",
    "    def get_item_users(self, item):\n",
    "        item_data = self.train_data[self.train_data[self.item_id] == item]\n",
    "        item_users = set(item_data[self.user_id].unique())\n",
    "            \n",
    "        return item_users\n",
    "        \n",
    "    #Get unique items (songs) in the training data\n",
    "    def get_all_items_train_data(self):\n",
    "        all_items = list(self.train_data[self.item_id].unique())\n",
    "            \n",
    "        return all_items\n",
    "        \n",
    "    #Construct cooccurence matrix\n",
    "    def construct_cooccurence_matrix(self, user_songs, all_songs):\n",
    "            \n",
    "        ####################################\n",
    "        #Get users for all songs in user_songs.\n",
    "        ####################################\n",
    "        user_songs_users = []        \n",
    "        for i in range(0, len(user_songs)):\n",
    "            user_songs_users.append(self.get_item_users(user_songs[i]))\n",
    "            \n",
    "        ###############################################\n",
    "        #Initialize the item cooccurence matrix of size \n",
    "        #len(user_songs) X len(songs)\n",
    "        ###############################################\n",
    "        cooccurence_matrix = np.matrix(np.zeros(shape=(len(user_songs), len(all_songs))), float)\n",
    "           \n",
    "        #############################################################\n",
    "        #Calculate similarity between user songs and all unique songs\n",
    "        #in the training data\n",
    "        #############################################################\n",
    "        for i in range(0,len(all_songs)):\n",
    "            #Calculate unique listeners (users) of song (item) i\n",
    "            songs_i_data = self.train_data[self.train_data[self.item_id] == all_songs[i]]\n",
    "            users_i = set(songs_i_data[self.user_id].unique())\n",
    "            \n",
    "            for j in range(0,len(user_songs)):       \n",
    "                    \n",
    "                #Get unique listeners (users) of song (item) j\n",
    "                users_j = user_songs_users[j]\n",
    "                    \n",
    "                #Calculate intersection of listeners of songs i and j\n",
    "                users_intersection = users_i.intersection(users_j)\n",
    "                \n",
    "                #Calculate cooccurence_matrix[i,j] as Jaccard Index\n",
    "                if len(users_intersection) != 0:\n",
    "                    #Calculate union of listeners of songs i and j\n",
    "                    users_union = users_i.union(users_j)\n",
    "                    \n",
    "                    cooccurence_matrix[j,i] = float(len(users_intersection))/float(len(users_union))\n",
    "                else:\n",
    "                    cooccurence_matrix[j,i] = 0\n",
    "                    \n",
    "        \n",
    "        return cooccurence_matrix\n",
    "\n",
    "    \n",
    "    #Use the cooccurence matrix to make top recommendations\n",
    "    def generate_top_recommendations(self, user, cooccurence_matrix, all_songs, user_songs):\n",
    "        print(\"Non zero values in cooccurence_matrix :%d\" % np.count_nonzero(cooccurence_matrix))\n",
    "        \n",
    "        #Calculate a weighted average of the scores in cooccurence matrix for all user songs.\n",
    "        user_sim_scores = cooccurence_matrix.sum(axis=0)/float(cooccurence_matrix.shape[0])\n",
    "        user_sim_scores = np.array(user_sim_scores)[0].tolist()\n",
    " \n",
    "        #Sort the indices of user_sim_scores based upon their value\n",
    "        #Also maintain the corresponding score\n",
    "        sort_index = sorted(((e,i) for i,e in enumerate(list(user_sim_scores))), reverse=True)\n",
    "    \n",
    "        #Create a dataframe from the following\n",
    "        columns = ['user_id', 'song', 'score', 'rank']\n",
    "        #index = np.arange(1) # array of numbers for the number of samples\n",
    "        df = pandas.DataFrame(columns=columns)\n",
    "         \n",
    "        #Fill the dataframe with top 10 item based recommendations\n",
    "        rank = 1 \n",
    "        for i in range(0,len(sort_index)):\n",
    "            if ~np.isnan(sort_index[i][0]) and all_songs[sort_index[i][1]] not in user_songs and rank <= 10:\n",
    "                df.loc[len(df)]=[user,all_songs[sort_index[i][1]],sort_index[i][0],rank]\n",
    "                rank = rank+1\n",
    "        \n",
    "        #Handle the case where there are no recommendations\n",
    "        if df.shape[0] == 0:\n",
    "            print(\"The current user has no songs for training the item similarity based recommendation model.\")\n",
    "            return -1\n",
    "        else:\n",
    "            return df\n",
    " \n",
    "    #Create the item similarity based recommender system model\n",
    "    def create(self, train_data, user_id, item_id):\n",
    "        self.train_data = train_data\n",
    "        self.user_id = user_id\n",
    "        self.item_id = item_id\n",
    "\n",
    "    #Use the item similarity based recommender system model to\n",
    "    #make recommendations\n",
    "    def recommend(self, user):\n",
    "        start = time.time()\n",
    "        \n",
    "        ########################################\n",
    "        #A. Get all unique songs for this user\n",
    "        ########################################\n",
    "        user_songs = self.get_user_items(user)    \n",
    "            \n",
    "        print(\"No. of unique songs for the user: %d\" % len(user_songs))\n",
    "        \n",
    "        ######################################################\n",
    "        #B. Get all unique items (songs) in the training data\n",
    "        ######################################################\n",
    "        all_songs = self.get_all_items_train_data()\n",
    "        \n",
    "        print(\"no. of unique songs in the training set: %d\" % len(all_songs))\n",
    "         \n",
    "        ###############################################\n",
    "        #C. Construct item cooccurence matrix of size \n",
    "        #len(user_songs) X len(songs)\n",
    "        ###############################################\n",
    "        cooccurence_matrix = self.construct_cooccurence_matrix(user_songs, all_songs)\n",
    "        \n",
    "        #######################################################\n",
    "        #D. Use the cooccurence matrix to make recommendations\n",
    "        #######################################################\n",
    "        df_recommendations = self.generate_top_recommendations(user, cooccurence_matrix, all_songs, user_songs)\n",
    "        \n",
    "        end = time.time()\n",
    "        print(end - start)\n",
    "        \n",
    "        return df_recommendations\n",
    "    \n",
    "    #Get similar items to given items\n",
    "    def get_similar_items(self, item_list):\n",
    "        \n",
    "        user_songs = item_list\n",
    "        \n",
    "        ######################################################\n",
    "        #B. Get all unique items (songs) in the training data\n",
    "        ######################################################\n",
    "        all_songs = self.get_all_items_train_data()\n",
    "        \n",
    "        print(\"no. of unique songs in the training set: %d\" % len(all_songs))\n",
    "         \n",
    "        ###############################################\n",
    "        #C. Construct item cooccurence matrix of size \n",
    "        #len(user_songs) X len(songs)\n",
    "        ###############################################\n",
    "        cooccurence_matrix = self.construct_cooccurence_matrix(user_songs, all_songs)\n",
    "        \n",
    "        #######################################################\n",
    "        #D. Use the cooccurence matrix to make recommendations\n",
    "        #######################################################\n",
    "        user = \"\"\n",
    "        df_recommendations = self.generate_top_recommendations(user, cooccurence_matrix, all_songs, user_songs)\n",
    "         \n",
    "        return df_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of item similarity based recommender class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_model = item_similarity_recommender_py()\n",
    "is_model.create(train_data, 'user_id', 'song')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the personalized model to make some song recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Print the songs for the user in training data\n",
    "user_id = '04396079bfe2a35ee92522dfadf2056ef899c456'\n",
    "user_items = is_model.get_user_items(user_id)\n",
    "#\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "print(\"Training data songs for the user userid: %s:\" % user_id)\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "\n",
    "for user_item in user_items:\n",
    "    print(user_item)\n",
    "\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Recommendation process going on:\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "#Recommend songs for the user using personalized model\n",
    "is_model.recommend(user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 3. Use the personalized model to make recommendations for the following user id. (Note the difference in recommendations from the first user id.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_id = 'b060d8ee0a018bc167f2feaaf9f50d5c84ac6ae4'\n",
    "#Fill in the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also apply the model to find similar songs to any song in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_model.get_similar_items(['The Scientist - Coldplay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 4. Use the personalized recommender model to get similar songs for the following song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "song = 'Easily (Album Version) - Red Hot Chili Peppers'\n",
    "###Fill in the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative comparison between the models\n",
    "\n",
    "We now formally compare the popularity and the personalized models using precision-recall curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class to calculate precision and recall (This can be used as a black box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Class to calculate precision and recall\n",
    "\n",
    "import random\n",
    "\n",
    "class precision_recall_calculator():\n",
    "    \n",
    "    def __init__(self, test_data, train_data):\n",
    "        self.test_data = test_data\n",
    "        self.train_data = train_data\n",
    "        self.user_test_sample = None\n",
    "        \n",
    "        self.ism_training_dict = dict()\n",
    "        self.pm_training_dict = dict()\n",
    "        self.test_dict = dict()\n",
    "    \n",
    "    #Method to return random percentage of values from a list\n",
    "    def remove_percentage(self, list_a, percentage):\n",
    "        k = int(len(list_a) * percentage)\n",
    "        random.seed(0)\n",
    "        indicies = random.sample(range(len(list_a)), k)\n",
    "        new_list = [list_a[i] for i in indicies]\n",
    "    \n",
    "        return new_list\n",
    "    \n",
    "    #Create a test sample of users for use in calculating precision\n",
    "    #and recall\n",
    "    def create_user_test_sample(self, percentage):\n",
    "        #Find users common between training and test set\n",
    "        users_test_and_training = list(set(self.test_data['user_id'].unique()).intersection(set(self.train_data['user_id'].unique())))\n",
    "        print(\"Length of user_test_and_training:%d\" % len(users_test_and_training))\n",
    "\n",
    "        #Take only random user_sample of users for evaluations\n",
    "        self.users_test_sample = self.remove_percentage(users_test_and_training, percentage)\n",
    "\n",
    "        print(\"Length of user sample:%d\" % len(self.users_test_sample))\n",
    "        \n",
    "    #Method to generate recommendations for users in the user test sample\n",
    "    def get_test_sample_recommendations(self):\n",
    "        #For these test_sample users, get top 10 recommendations from training set\n",
    "        #self.ism_training_dict = {}\n",
    "        #self.pm_training_dict = {}\n",
    "\n",
    "        #self.test_dict = {}\n",
    "\n",
    "        for user_id in self.users_test_sample:\n",
    "            #Get items for user_id from item similarity model\n",
    "            print(\"Getting recommendations for user:%s\" % user_id)\n",
    "            user_sim_items = is_model.recommend(user_id)\n",
    "            self.ism_training_dict[user_id] = list(user_sim_items[\"song\"])\n",
    "    \n",
    "            #Get items for user_id from popularity model\n",
    "            user_sim_items = pm.recommend(user_id)\n",
    "            self.pm_training_dict[user_id] = list(user_sim_items[\"song\"])\n",
    "    \n",
    "            #Get items for user_id from test_data\n",
    "            test_data_user = self.test_data[self.test_data['user_id'] == user_id]\n",
    "            self.test_dict[user_id] = set(test_data_user['song'].unique() )\n",
    "    \n",
    "    #Method to calculate the precision and recall measures\n",
    "    def calculate_precision_recall(self):\n",
    "        #Create cutoff list for precision and recall calculation\n",
    "        cutoff_list = list(range(1,11))\n",
    "\n",
    "\n",
    "        #For each distinct cutoff:\n",
    "        #    1. For each distinct user, calculate precision and recall.\n",
    "        #    2. Calculate average precision and recall.\n",
    "\n",
    "        ism_avg_precision_list = []\n",
    "        ism_avg_recall_list = []\n",
    "        pm_avg_precision_list = []\n",
    "        pm_avg_recall_list = []\n",
    "\n",
    "\n",
    "        num_users_sample = len(self.users_test_sample)\n",
    "        for N in cutoff_list:\n",
    "            ism_sum_precision = 0\n",
    "            ism_sum_recall = 0\n",
    "            pm_sum_precision = 0\n",
    "            pm_sum_recall = 0\n",
    "            ism_avg_precision = 0\n",
    "            ism_avg_recall = 0\n",
    "            pm_avg_precision = 0\n",
    "            pm_avg_recall = 0\n",
    "\n",
    "            for user_id in self.users_test_sample:\n",
    "                ism_hitset = self.test_dict[user_id].intersection(set(self.ism_training_dict[user_id][0:N]))\n",
    "                pm_hitset = self.test_dict[user_id].intersection(set(self.pm_training_dict[user_id][0:N]))\n",
    "                testset = self.test_dict[user_id]\n",
    "        \n",
    "                pm_sum_precision += float(len(pm_hitset))/float(N)\n",
    "                pm_sum_recall += float(len(pm_hitset))/float(len(testset))\n",
    "\n",
    "                ism_sum_precision += float(len(ism_hitset))/float(len(testset))\n",
    "                ism_sum_recall += float(len(ism_hitset))/float(N)\n",
    "        \n",
    "            pm_avg_precision = pm_sum_precision/float(num_users_sample)\n",
    "            pm_avg_recall = pm_sum_recall/float(num_users_sample)\n",
    "    \n",
    "            ism_avg_precision = ism_sum_precision/float(num_users_sample)\n",
    "            ism_avg_recall = ism_sum_recall/float(num_users_sample)\n",
    "\n",
    "            ism_avg_precision_list.append(ism_avg_precision)\n",
    "            ism_avg_recall_list.append(ism_avg_recall)\n",
    "    \n",
    "            pm_avg_precision_list.append(pm_avg_precision)\n",
    "            pm_avg_recall_list.append(pm_avg_recall)\n",
    "            \n",
    "        return (pm_avg_precision_list, pm_avg_recall_list, ism_avg_precision_list, ism_avg_recall_list)\n",
    "     \n",
    "\n",
    "    #A wrapper method to calculate all the evaluation measures\n",
    "    def calculate_measures(self, percentage):\n",
    "        #Create a test sample of users\n",
    "        self.create_user_test_sample(percentage)\n",
    "        \n",
    "        #Generate recommendations for the test sample users\n",
    "        self.get_test_sample_recommendations()\n",
    "        \n",
    "        #Calculate precision and recall at different cutoff values\n",
    "        #for popularity mode (pm) as well as item similarity model (ism)\n",
    "        \n",
    "        return self.calculate_precision_recall()\n",
    "        #return (pm_avg_precision_list, pm_avg_recall_list, ism_avg_precision_list, ism_avg_recall_list)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the above precision recall calculator class to calculate the evaluation measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "#Define what percentage of users to use for precision recall calculation\n",
    "user_sample = 0.04\n",
    "\n",
    "#Instantiate the precision_recall_calculator class\n",
    "pr = precision_recall_calculator(test_data, train_data)\n",
    "\n",
    "#Call method to calculate precision and recall values\n",
    "(pm_avg_precision_list, pm_avg_recall_list, ism_avg_precision_list, ism_avg_recall_list) = pr.calculate_measures(user_sample)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to plot precision recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "\n",
    "#Method to generate precision and recall curve\n",
    "def plot_precision_recall(m1_precision_list, m1_recall_list, m1_label, m2_precision_list, m2_recall_list, m2_label):\n",
    "    pl.clf()    \n",
    "    pl.plot(m1_recall_list, m1_precision_list, label=m1_label)\n",
    "    pl.plot(m2_recall_list, m2_precision_list, label=m2_label)\n",
    "    pl.xlabel('Recall')\n",
    "    pl.ylabel('Precision')\n",
    "    pl.ylim([0.0, 0.20])\n",
    "    pl.xlim([0.0, 0.20])\n",
    "    pl.title('Precision-Recall curve')\n",
    "    pl.legend(loc=\"upper right\")\n",
    "    pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Plotting precision recall curves.\")\n",
    "\n",
    "plot_precision_recall(pm_avg_precision_list, pm_avg_recall_list, \"popularity_model\",\n",
    "                      ism_avg_precision_list, ism_avg_recall_list, \"item_similarity_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The curve shows that the personalized model provides much better performance over the popularity model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
